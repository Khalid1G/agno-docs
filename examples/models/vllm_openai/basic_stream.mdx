---
title: Streaming Agent
---

Make sure to start the vLLM server first:

```shell
vllm serve Qwen/Qwen2.5-7B-Instruct \
    --enable-auto-tool-choice \
    --tool-call-parser hermes \
    --dtype float16 \
    --max-model-len 8192 \
    --gpu-memory-utilization 0.9 \
    --trust-remote-code
```

## Code

```python cookbook/models/vllm/basic_stream.py
from agno.agent import Agent
from agno.models.vllm import vLLM

agent = Agent(
    model=vLLM(id="Qwen/Qwen2.5-7B-Instruct", top_k=20, enable_thinking=False),
    markdown=True,
)
agent.print_response("Share a 2 sentence horror story", stream=True)
```

## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />
  
  <Step title="Install packages">
    ```bash
    pip install openai agno
    ```
  </Step>
  
  <Step title="Export vLLM API key">
    ```bash
    export VLLM_API_KEY="EMPTY"
    ```
  </Step>
  
  <Step title="Export vLLM host">
    ```bash
    export VLLM_BASE_URL="http://localhost:8000/v1/"
    ```
  </Step>
  
  <Step title="Run Agent">
    <CodeGroup>
    ```bash Mac/Linux
    python cookbook/models/vllm/basic_stream.py
    ```
    ```bash Windows
    python cookbook/models/vllm/basic_stream.py
    ```
    </CodeGroup>
  </Step>
</Steps>